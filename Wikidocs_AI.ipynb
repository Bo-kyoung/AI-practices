{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wikidocs AI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvBKaEYX8c6N954P1/kZfz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bo-kyoung/AI-practices/blob/main/Wikidocs_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Calculator:\n",
        "  def __init__(self):\n",
        "    self.result=0\n",
        "    print(\"선언완료\")\n",
        "  def add(self, num):\n",
        "    self.result += num\n",
        "    return self.result\n",
        "cal1= Calculator()\n",
        "cal2= Calculator()\n",
        "print(cal1.add(3))\n",
        "print(cal1.add(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXrytLxC_6wR",
        "outputId": "f051067d-8790-407e-dc94-4de1eacf20ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "선언완료\n",
            "선언완료\n",
            "3\n",
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1)\n",
        "x_train = torch.FloatTensor([1,2,3]).unsqueeze(1)\n",
        "y_train = torch.FloatTensor([2,4,6]).unsqueeze(1)\n",
        "\n",
        "W = torch.zeros(1, requires_grad=True) # X*W이므로 (1*1)\n",
        "b = torch.zeros(1, requires_grad=True) \n",
        "\n",
        "nb_epochs = 2999\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  hypothesis = x_train.mul(W)+b \n",
        "\n",
        "\n",
        "  cost = torch.mean((hypothesis - y_train)**2)\n",
        "\n",
        "\n",
        "  optimizer = optim.SGD([W,b], lr = 0.01)\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward() #기울기 연산(hypothesis에 초기 w, b대입, 계산, cost에 hypothesis 대입, 계산, 해서 w, b(required step TRUE 설정) return \n",
        "  optimizer.step() #learnin late 계산 해서 기울기를 빼줌\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print('Epoch {:4d}/{} W: {:.4f}, b: {:.3f} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
        "        ))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-ngqSMrAZab",
        "outputId": "e29059d1-c343-4535-e86f-73250b5da5f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0/2999 W: 0.1867, b: 0.080 Cost: 18.666666\n",
            "Epoch 100/2999 W: 1.7457, b: 0.578 Cost: 0.048171\n",
            "Epoch 200/2999 W: 1.8001, b: 0.454 Cost: 0.029767\n",
            "Epoch 300/2999 W: 1.8429, b: 0.357 Cost: 0.018394\n",
            "Epoch 400/2999 W: 1.8765, b: 0.281 Cost: 0.011366\n",
            "Epoch 500/2999 W: 1.9029, b: 0.221 Cost: 0.007024\n",
            "Epoch 600/2999 W: 1.9237, b: 0.174 Cost: 0.004340\n",
            "Epoch 700/2999 W: 1.9400, b: 0.136 Cost: 0.002682\n",
            "Epoch 800/2999 W: 1.9528, b: 0.107 Cost: 0.001657\n",
            "Epoch 900/2999 W: 1.9629, b: 0.084 Cost: 0.001024\n",
            "Epoch 1000/2999 W: 1.9709, b: 0.066 Cost: 0.000633\n",
            "Epoch 1100/2999 W: 1.9771, b: 0.052 Cost: 0.000391\n",
            "Epoch 1200/2999 W: 1.9820, b: 0.041 Cost: 0.000242\n",
            "Epoch 1300/2999 W: 1.9858, b: 0.032 Cost: 0.000149\n",
            "Epoch 1400/2999 W: 1.9889, b: 0.025 Cost: 0.000092\n",
            "Epoch 1500/2999 W: 1.9913, b: 0.020 Cost: 0.000057\n",
            "Epoch 1600/2999 W: 1.9931, b: 0.016 Cost: 0.000035\n",
            "Epoch 1700/2999 W: 1.9946, b: 0.012 Cost: 0.000022\n",
            "Epoch 1800/2999 W: 1.9958, b: 0.010 Cost: 0.000013\n",
            "Epoch 1900/2999 W: 1.9967, b: 0.008 Cost: 0.000008\n",
            "Epoch 2000/2999 W: 1.9974, b: 0.006 Cost: 0.000005\n",
            "Epoch 2100/2999 W: 1.9979, b: 0.005 Cost: 0.000003\n",
            "Epoch 2200/2999 W: 1.9984, b: 0.004 Cost: 0.000002\n",
            "Epoch 2300/2999 W: 1.9987, b: 0.003 Cost: 0.000001\n",
            "Epoch 2400/2999 W: 1.9990, b: 0.002 Cost: 0.000001\n",
            "Epoch 2500/2999 W: 1.9992, b: 0.002 Cost: 0.000000\n",
            "Epoch 2600/2999 W: 1.9994, b: 0.001 Cost: 0.000000\n",
            "Epoch 2700/2999 W: 1.9995, b: 0.001 Cost: 0.000000\n",
            "Epoch 2800/2999 W: 1.9996, b: 0.001 Cost: 0.000000\n",
            "Epoch 2900/2999 W: 1.9997, b: 0.001 Cost: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.tensor(2.0, requires_grad  = True)\n",
        "y = w**2\n",
        "z = 2*y + 5\n",
        "z.backward()\n",
        "print(w.grad) #미분한 수식: f(x) = 4w, w값은 2였으니 기울기는 8 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0qIMUIkUAqE",
        "outputId": "3e7c764a-a7f3-4645-ac44-3f00429787ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
        "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
        "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]]) #feature가 3개(x의 개수 - dim이 3)\n",
        "\n",
        "w1 = torch.zeros(1, requires_grad=True)\n",
        "w2 = torch.zeros(1, requires_grad=True)\n",
        "w3 = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
        "\n",
        "    # cost 계산\n",
        "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
        "        ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "834EFKsqU1v_",
        "outputId": "23f094de-5874-4a63-e57d-fe126f64bdde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
            "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563628\n",
            "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497595\n",
            "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435044\n",
            "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375726\n",
            "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319507\n",
            "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
            "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215703\n",
            "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167810\n",
            "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
            "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "                               [93,  88,  93], \n",
        "                               [89,  91,  80], \n",
        "                               [96,  98,  100],   \n",
        "                               [73,  66,  70]])  \n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
        "\n",
        "W = torch.zeros((3,1), requires_grad = True )\n",
        "b = torch.zeros(1, requires_grad = True)\n",
        "\n",
        "optimizer = optim.SGD([W,b], lr = 1e-5)\n",
        "nb_epochs = 20\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  hypothesis = x_train.matmul(W) + b\n",
        "  cost = torch.mean((hypothesis - y_train)**2)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
        "      ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la58QINKTQ2e",
        "outputId": "9045c2ba-b975-4bf2-f6d2-1b56b39632ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
            "Epoch    1/20 hypothesis: tensor([66.7178, 80.1701, 76.1025, 86.0194, 61.1565]) Cost: 9537.694336\n",
            "Epoch    2/20 hypothesis: tensor([104.5421, 125.6208, 119.2478, 134.7862,  95.8280]) Cost: 3069.590088\n",
            "Epoch    3/20 hypothesis: tensor([125.9858, 151.3882, 143.7087, 162.4333, 115.4844]) Cost: 990.670288\n",
            "Epoch    4/20 hypothesis: tensor([138.1429, 165.9963, 157.5768, 178.1071, 126.6283]) Cost: 322.481873\n",
            "Epoch    5/20 hypothesis: tensor([145.0350, 174.2780, 165.4395, 186.9928, 132.9461]) Cost: 107.717064\n",
            "Epoch    6/20 hypothesis: tensor([148.9423, 178.9730, 169.8976, 192.0301, 136.5279]) Cost: 38.687496\n",
            "Epoch    7/20 hypothesis: tensor([151.1574, 181.6346, 172.4254, 194.8856, 138.5585]) Cost: 16.499043\n",
            "Epoch    8/20 hypothesis: tensor([152.4131, 183.1435, 173.8590, 196.5043, 139.7097]) Cost: 9.365656\n",
            "Epoch    9/20 hypothesis: tensor([153.1250, 183.9988, 174.6723, 197.4217, 140.3625]) Cost: 7.071114\n",
            "Epoch   10/20 hypothesis: tensor([153.5285, 184.4835, 175.1338, 197.9415, 140.7325]) Cost: 6.331847\n",
            "Epoch   11/20 hypothesis: tensor([153.7572, 184.7582, 175.3958, 198.2360, 140.9424]) Cost: 6.092532\n",
            "Epoch   12/20 hypothesis: tensor([153.8868, 184.9138, 175.5449, 198.4026, 141.0613]) Cost: 6.013817\n",
            "Epoch   13/20 hypothesis: tensor([153.9602, 185.0019, 175.6299, 198.4969, 141.1288]) Cost: 5.986785\n",
            "Epoch   14/20 hypothesis: tensor([154.0017, 185.0517, 175.6785, 198.5500, 141.1671]) Cost: 5.976325\n",
            "Epoch   15/20 hypothesis: tensor([154.0252, 185.0798, 175.7065, 198.5800, 141.1888]) Cost: 5.971208\n",
            "Epoch   16/20 hypothesis: tensor([154.0385, 185.0956, 175.7229, 198.5966, 141.2012]) Cost: 5.967835\n",
            "Epoch   17/20 hypothesis: tensor([154.0459, 185.1045, 175.7326, 198.6059, 141.2082]) Cost: 5.964969\n",
            "Epoch   18/20 hypothesis: tensor([154.0501, 185.1094, 175.7386, 198.6108, 141.2122]) Cost: 5.962291\n",
            "Epoch   19/20 hypothesis: tensor([154.0524, 185.1120, 175.7424, 198.6134, 141.2145]) Cost: 5.959664\n",
            "Epoch   20/20 hypothesis: tensor([154.0536, 185.1134, 175.7451, 198.6145, 141.2158]) Cost: 5.957089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nn.module\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1)\n",
        "x_train = torch.FloatTensor([1,2,3]).unsqueeze(1)\n",
        "y_train = torch.FloatTensor([2,4,6]).unsqueeze(1)\n",
        "print(x_train)\n",
        "\n",
        "model = nn.Linear(1,1)\n",
        "#print(list(model.parameters()))\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01 )\n",
        "nb_epochs = 2000\n",
        "\n",
        "for epoch in range(nb_epochs+1):\n",
        "  prediction = model(x_train)\n",
        "  cost = F.mse_loss(prediction, y_train)\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "     print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, cost.item()\n",
        "      ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaupJ5OAXKxq",
        "outputId": "892c4301-866e-4196-8c2d-f84ebdd009ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "Epoch    0/2000 Cost: 13.103541\n",
            "Epoch  100/2000 Cost: 0.002791\n",
            "Epoch  200/2000 Cost: 0.001724\n",
            "Epoch  300/2000 Cost: 0.001066\n",
            "Epoch  400/2000 Cost: 0.000658\n",
            "Epoch  500/2000 Cost: 0.000407\n",
            "Epoch  600/2000 Cost: 0.000251\n",
            "Epoch  700/2000 Cost: 0.000155\n",
            "Epoch  800/2000 Cost: 0.000096\n",
            "Epoch  900/2000 Cost: 0.000059\n",
            "Epoch 1000/2000 Cost: 0.000037\n",
            "Epoch 1100/2000 Cost: 0.000023\n",
            "Epoch 1200/2000 Cost: 0.000014\n",
            "Epoch 1300/2000 Cost: 0.000009\n",
            "Epoch 1400/2000 Cost: 0.000005\n",
            "Epoch 1500/2000 Cost: 0.000003\n",
            "Epoch 1600/2000 Cost: 0.000002\n",
            "Epoch 1700/2000 Cost: 0.000001\n",
            "Epoch 1800/2000 Cost: 0.000001\n",
            "Epoch 1900/2000 Cost: 0.000000\n",
            "Epoch 2000/2000 Cost: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_var = torch.FloatTensor([[4.0]])\n",
        "pred_y = model(new_var)\n",
        "print(pred_y)\n",
        "print(list(model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIvnoNaKesmF",
        "outputId": "ade97108-ed92-46d2-de89-32f682fbbe49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[7.9989]], grad_fn=<AddmmBackward0>)\n",
            "[Parameter containing:\n",
            "tensor([[1.9994]], requires_grad=True), Parameter containing:\n",
            "tensor([0.0014], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "model = nn.Linear(3,1) #3개의 feature 하나의 y\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
        "nb_epochs = 2000\n",
        "for epoch in range(nb_epochs+1):\n",
        "  prediction = model(x_train)\n",
        "  cost = F.mse_loss(prediction, y_train)\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 100 == 0:\n",
        "    # 100번마다 로그 출력\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, cost.item()\n",
        "      ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDzXRekefCxc",
        "outputId": "202124cd-bf34-428f-8781-ac91f2240dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/2000 Cost: 29938.339844\n",
            "Epoch  100/2000 Cost: 1.459696\n",
            "Epoch  200/2000 Cost: 1.404118\n",
            "Epoch  300/2000 Cost: 1.351442\n",
            "Epoch  400/2000 Cost: 1.301445\n",
            "Epoch  500/2000 Cost: 1.254035\n",
            "Epoch  600/2000 Cost: 1.209076\n",
            "Epoch  700/2000 Cost: 1.166424\n",
            "Epoch  800/2000 Cost: 1.125966\n",
            "Epoch  900/2000 Cost: 1.087573\n",
            "Epoch 1000/2000 Cost: 1.051147\n",
            "Epoch 1100/2000 Cost: 1.016586\n",
            "Epoch 1200/2000 Cost: 0.983798\n",
            "Epoch 1300/2000 Cost: 0.952660\n",
            "Epoch 1400/2000 Cost: 0.923131\n",
            "Epoch 1500/2000 Cost: 0.895094\n",
            "Epoch 1600/2000 Cost: 0.868473\n",
            "Epoch 1700/2000 Cost: 0.843199\n",
            "Epoch 1800/2000 Cost: 0.819206\n",
            "Epoch 1900/2000 Cost: 0.796417\n",
            "Epoch 2000/2000 Cost: 0.774786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(1,1)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return self.linear(x)\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])\n",
        "model = LinearRegressionModel()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
        "nb_epochs = 2000\n",
        "for epoch in range(nb_epochs+1):\n",
        "  prediction = model(x_train)\n",
        "  cost = F.mse_loss(prediction, y_train)\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 100 == 0:\n",
        "    # 100번마다 로그 출력\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, cost.item()\n",
        "      ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trQyzEQUkaYZ",
        "outputId": "d6972a57-1268-4df0-86e8-87c31c3549b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/2000 Cost: 32.361588\n",
            "Epoch  100/2000 Cost: 0.012030\n",
            "Epoch  200/2000 Cost: 0.007434\n",
            "Epoch  300/2000 Cost: 0.004593\n",
            "Epoch  400/2000 Cost: 0.002839\n",
            "Epoch  500/2000 Cost: 0.001754\n",
            "Epoch  600/2000 Cost: 0.001084\n",
            "Epoch  700/2000 Cost: 0.000670\n",
            "Epoch  800/2000 Cost: 0.000414\n",
            "Epoch  900/2000 Cost: 0.000256\n",
            "Epoch 1000/2000 Cost: 0.000158\n",
            "Epoch 1100/2000 Cost: 0.000098\n",
            "Epoch 1200/2000 Cost: 0.000060\n",
            "Epoch 1300/2000 Cost: 0.000037\n",
            "Epoch 1400/2000 Cost: 0.000023\n",
            "Epoch 1500/2000 Cost: 0.000014\n",
            "Epoch 1600/2000 Cost: 0.000009\n",
            "Epoch 1700/2000 Cost: 0.000005\n",
            "Epoch 1800/2000 Cost: 0.000003\n",
            "Epoch 1900/2000 Cost: 0.000002\n",
            "Epoch 2000/2000 Cost: 0.000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "class MultivariateLinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "model = MultivariateLinearRegressionModel()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
        "nb_epochs = 2000\n",
        "for epoch in range(nb_epochs +1):\n",
        "  prediction = model(x_train)\n",
        "  cost = F.mse_loss(prediction, y_train)\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 100 == 0:\n",
        "    # 100번마다 로그 출력\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, cost.item()\n",
        "      ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4bD9YpJl9dL",
        "outputId": "07c6f9be-09b6-496a-b2d9-055b1b085cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/2000 Cost: 25809.587891\n",
            "Epoch  100/2000 Cost: 0.298808\n",
            "Epoch  200/2000 Cost: 0.292559\n",
            "Epoch  300/2000 Cost: 0.286632\n",
            "Epoch  400/2000 Cost: 0.281008\n",
            "Epoch  500/2000 Cost: 0.275684\n",
            "Epoch  600/2000 Cost: 0.270632\n",
            "Epoch  700/2000 Cost: 0.265832\n",
            "Epoch  800/2000 Cost: 0.261289\n",
            "Epoch  900/2000 Cost: 0.256981\n",
            "Epoch 1000/2000 Cost: 0.252894\n",
            "Epoch 1100/2000 Cost: 0.249015\n",
            "Epoch 1200/2000 Cost: 0.245331\n",
            "Epoch 1300/2000 Cost: 0.241845\n",
            "Epoch 1400/2000 Cost: 0.238542\n",
            "Epoch 1500/2000 Cost: 0.235392\n",
            "Epoch 1600/2000 Cost: 0.232420\n",
            "Epoch 1700/2000 Cost: 0.229590\n",
            "Epoch 1800/2000 Cost: 0.226897\n",
            "Epoch 1900/2000 Cost: 0.224347\n",
            "Epoch 2000/2000 Cost: 0.221932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "                               [93,  88,  93], \n",
        "                               [89,  91,  90], \n",
        "                               [96,  98,  100],   \n",
        "                               [73,  66,  70]])  \n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
        "\n",
        "dataset = TensorDataset(x_train, y_train) #dataloader(인자로 dataset과 batch크기를 받음)를 위한\n",
        "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)\n",
        "model = nn.Linear(3,1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
        "nb_epochs = 20\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  for batch_idx, sample in enumerate(dataloader): #enumerate() 함수는 기본적으로 인덱스와 원소로 이루어진 터플(tuple)을 만들어줌\n",
        "    #print(batch_idx)\n",
        "    #print(sample)\n",
        "    x_train, y_train = sample\n",
        "    prediction = model(x_train)\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
        "        cost.item()\n",
        "      ))\n",
        "    "
      ],
      "metadata": {
        "id": "3FMJm9ZpsJb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_var = torch.FloatTensor([[73,80,75]])\n",
        "pred_y = model(new_var)\n",
        "print(pred_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndS2pFJUZjil",
        "outputId": "68f4eec9-aa06-438d-86d6-fb47d222b80a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[154.1705]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = [[73, 80, 75],\n",
        "                   [93, 88, 93],\n",
        "                   [89, 91, 90],\n",
        "                   [96, 98, 100],\n",
        "                   [73, 66, 70]]\n",
        "    self.y_data = [[152], [185], [180], [196], [142]]\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx])\n",
        "    y = torch.FloatTensor(self.y_data[idx])\n",
        "    return x,y\n",
        "\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)\n",
        "model = torch.nn.Linear(3,1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
        "nb_epochs = 2000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    # print(batch_idx)\n",
        "    # print(samples)\n",
        "    x_train, y_train = samples\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "    # cost로 H(x) 계산\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
        "        cost.item()\n",
        "        ))"
      ],
      "metadata": {
        "id": "1FSuTqIzZ0V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "x = np.arange(-5.0, 5.0, 0.1)\n",
        "y1 = sigmoid(0.5*x)\n",
        "y2 = sigmoid(x)\n",
        "y3 = sigmoid(2*x)\n",
        "\n",
        "plt.plot(x, y1, 'r', linestyle='--') # W의 값이 0.5일때\n",
        "plt.plot(x, y2, 'g') # W의 값이 1일때\n",
        "plt.plot(x, y3, 'b', linestyle='--') # W의 값이 2일때\n",
        "plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\n",
        "plt.title('Sigmoid Function')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "aLGtshX_lAUI",
        "outputId": "eeb8accb-168a-48c9-8b03-1cf6ff4c9437"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
        "y_data = [[0], [0], [0], [1], [1], [1]]\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    \n",
        "    nn.Linear(2,1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "model(x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I3HU1mSkUuI",
        "outputId": "50d86fa7-e7eb-4919-c520-961454a1a23b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4020],\n",
              "        [0.4147],\n",
              "        [0.6556],\n",
              "        [0.5948],\n",
              "        [0.6788],\n",
              "        [0.8061]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
        "y_data = [[0], [0], [0], [1], [1], [1]]\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)\n",
        "\n",
        "W = torch.zeros((2, 1), requires_grad=True) # 크기는 2 x 1\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))\n",
        "print(hypothesis)\n",
        "hypothesis = torch.sigmoid(x_train.matmul(W)+b)\n",
        "print(hypothesis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIOQneGVnTU9",
        "outputId": "0920df35-c60e-4a33-b974-caa4e83df423"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000]], grad_fn=<MulBackward0>)\n",
            "tensor([[0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "-(y_train[0]*torch.log(hypothesis[0]))+(1-y_train[0])*torch.log(1-hypothesis[0])\n",
        "losses = -(y_train * torch.log(hypothesis) + \n",
        "           (1 - y_train) * torch.log(1 - hypothesis))\n",
        "print(losses)\n",
        "cost = losses.mean()\n",
        "print(cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrKxTPQ1oNoh",
        "outputId": "b1275233-6cf8-4ddd-8a70-11bccde82298"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6931],\n",
            "        [0.6931],\n",
            "        [0.6931],\n",
            "        [0.6931],\n",
            "        [0.6931],\n",
            "        [0.6931]], grad_fn=<NegBackward0>)\n",
            "tensor(0.6931, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.binary_cross_entropy(hypothesis, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_imDrsyokyY",
        "outputId": "65094d5d-8e6d-4669-9663-a3fb708e3ac9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.zeros((2, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  hypothesis = torch.sigmoid(x_train.matmul(W)+b)\n",
        "  cost = -(y_train * torch.log(hypothesis) + \n",
        "           (1 - y_train) * torch.log(1 - hypothesis)).mean()\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 100 == 0:\n",
        "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, cost.item()\n",
        "        ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB7AwmL8o4nQ",
        "outputId": "23edccc8-c249-42f8-9e2c-7adc184745ef"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/1000 Cost: 0.693147\n",
            "Epoch  100/1000 Cost: 0.134722\n",
            "Epoch  200/1000 Cost: 0.080643\n",
            "Epoch  300/1000 Cost: 0.057900\n",
            "Epoch  400/1000 Cost: 0.045300\n",
            "Epoch  500/1000 Cost: 0.037261\n",
            "Epoch  600/1000 Cost: 0.031673\n",
            "Epoch  700/1000 Cost: 0.027556\n",
            "Epoch  800/1000 Cost: 0.024394\n",
            "Epoch  900/1000 Cost: 0.021888\n",
            "Epoch 1000/1000 Cost: 0.019852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(hypothesis)\n",
        "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
        "print(W)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuo7b6EvpQcJ",
        "outputId": "37121fea-c23d-4d8d-86c3-7f9055ef2d8c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.7711e-04],\n",
            "        [3.1636e-02],\n",
            "        [3.9014e-02],\n",
            "        [9.5618e-01],\n",
            "        [9.9823e-01],\n",
            "        [9.9969e-01]], grad_fn=<SigmoidBackward0>)\n",
            "tensor([[3.2530],\n",
            "        [1.5179]], requires_grad=True)\n",
            "tensor([-14.4819], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1)\n",
        "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
        "y_data = [[0], [0], [0], [1], [1], [1]]\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)\n",
        "model = nn.Sequential(\n",
        "   nn.Linear(2, 1), # input_dim = 2, output_dim = 1\n",
        "   nn.Sigmoid() # 출력은 시그모이드 함수를 거친다\n",
        ")\n",
        "model(x_train)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1)\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs+1):\n",
        "  hypothesis = model(x_train)\n",
        "  cost = F.binary_cross_entropy(hypothesis, y_train)\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    prediction = hypothesis >= torch.FloatTensor([0.5])\n",
        "    correct_prediction = prediction.float() == y_train \n",
        "    accuracy = correct_prediction.sum().item() / len(correct_prediction) \n",
        "    print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format(\n",
        "        epoch, nb_epochs, cost.item(), accuracy * 100,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHViV7mqp94p",
        "outputId": "3805dbb6-6abf-4ee2-9e53-807ee4ab13a9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/1000 Cost: 0.539713 Accuracy 83.33%\n",
            "Epoch   10/1000 Cost: 0.614853 Accuracy 66.67%\n",
            "Epoch   20/1000 Cost: 0.441875 Accuracy 66.67%\n",
            "Epoch   30/1000 Cost: 0.373145 Accuracy 83.33%\n",
            "Epoch   40/1000 Cost: 0.316358 Accuracy 83.33%\n",
            "Epoch   50/1000 Cost: 0.266094 Accuracy 83.33%\n",
            "Epoch   60/1000 Cost: 0.220498 Accuracy 100.00%\n",
            "Epoch   70/1000 Cost: 0.182095 Accuracy 100.00%\n",
            "Epoch   80/1000 Cost: 0.157299 Accuracy 100.00%\n",
            "Epoch   90/1000 Cost: 0.144091 Accuracy 100.00%\n",
            "Epoch  100/1000 Cost: 0.134272 Accuracy 100.00%\n",
            "Epoch  110/1000 Cost: 0.125769 Accuracy 100.00%\n",
            "Epoch  120/1000 Cost: 0.118297 Accuracy 100.00%\n",
            "Epoch  130/1000 Cost: 0.111680 Accuracy 100.00%\n",
            "Epoch  140/1000 Cost: 0.105779 Accuracy 100.00%\n",
            "Epoch  150/1000 Cost: 0.100483 Accuracy 100.00%\n",
            "Epoch  160/1000 Cost: 0.095704 Accuracy 100.00%\n",
            "Epoch  170/1000 Cost: 0.091369 Accuracy 100.00%\n",
            "Epoch  180/1000 Cost: 0.087420 Accuracy 100.00%\n",
            "Epoch  190/1000 Cost: 0.083806 Accuracy 100.00%\n",
            "Epoch  200/1000 Cost: 0.080486 Accuracy 100.00%\n",
            "Epoch  210/1000 Cost: 0.077425 Accuracy 100.00%\n",
            "Epoch  220/1000 Cost: 0.074595 Accuracy 100.00%\n",
            "Epoch  230/1000 Cost: 0.071969 Accuracy 100.00%\n",
            "Epoch  240/1000 Cost: 0.069526 Accuracy 100.00%\n",
            "Epoch  250/1000 Cost: 0.067248 Accuracy 100.00%\n",
            "Epoch  260/1000 Cost: 0.065118 Accuracy 100.00%\n",
            "Epoch  270/1000 Cost: 0.063122 Accuracy 100.00%\n",
            "Epoch  280/1000 Cost: 0.061247 Accuracy 100.00%\n",
            "Epoch  290/1000 Cost: 0.059483 Accuracy 100.00%\n",
            "Epoch  300/1000 Cost: 0.057820 Accuracy 100.00%\n",
            "Epoch  310/1000 Cost: 0.056250 Accuracy 100.00%\n",
            "Epoch  320/1000 Cost: 0.054764 Accuracy 100.00%\n",
            "Epoch  330/1000 Cost: 0.053357 Accuracy 100.00%\n",
            "Epoch  340/1000 Cost: 0.052022 Accuracy 100.00%\n",
            "Epoch  350/1000 Cost: 0.050753 Accuracy 100.00%\n",
            "Epoch  360/1000 Cost: 0.049546 Accuracy 100.00%\n",
            "Epoch  370/1000 Cost: 0.048396 Accuracy 100.00%\n",
            "Epoch  380/1000 Cost: 0.047299 Accuracy 100.00%\n",
            "Epoch  390/1000 Cost: 0.046252 Accuracy 100.00%\n",
            "Epoch  400/1000 Cost: 0.045251 Accuracy 100.00%\n",
            "Epoch  410/1000 Cost: 0.044294 Accuracy 100.00%\n",
            "Epoch  420/1000 Cost: 0.043376 Accuracy 100.00%\n",
            "Epoch  430/1000 Cost: 0.042497 Accuracy 100.00%\n",
            "Epoch  440/1000 Cost: 0.041653 Accuracy 100.00%\n",
            "Epoch  450/1000 Cost: 0.040843 Accuracy 100.00%\n",
            "Epoch  460/1000 Cost: 0.040064 Accuracy 100.00%\n",
            "Epoch  470/1000 Cost: 0.039315 Accuracy 100.00%\n",
            "Epoch  480/1000 Cost: 0.038593 Accuracy 100.00%\n",
            "Epoch  490/1000 Cost: 0.037898 Accuracy 100.00%\n",
            "Epoch  500/1000 Cost: 0.037228 Accuracy 100.00%\n",
            "Epoch  510/1000 Cost: 0.036582 Accuracy 100.00%\n",
            "Epoch  520/1000 Cost: 0.035958 Accuracy 100.00%\n",
            "Epoch  530/1000 Cost: 0.035356 Accuracy 100.00%\n",
            "Epoch  540/1000 Cost: 0.034773 Accuracy 100.00%\n",
            "Epoch  550/1000 Cost: 0.034210 Accuracy 100.00%\n",
            "Epoch  560/1000 Cost: 0.033664 Accuracy 100.00%\n",
            "Epoch  570/1000 Cost: 0.033137 Accuracy 100.00%\n",
            "Epoch  580/1000 Cost: 0.032625 Accuracy 100.00%\n",
            "Epoch  590/1000 Cost: 0.032130 Accuracy 100.00%\n",
            "Epoch  600/1000 Cost: 0.031649 Accuracy 100.00%\n",
            "Epoch  610/1000 Cost: 0.031183 Accuracy 100.00%\n",
            "Epoch  620/1000 Cost: 0.030730 Accuracy 100.00%\n",
            "Epoch  630/1000 Cost: 0.030291 Accuracy 100.00%\n",
            "Epoch  640/1000 Cost: 0.029864 Accuracy 100.00%\n",
            "Epoch  650/1000 Cost: 0.029449 Accuracy 100.00%\n",
            "Epoch  660/1000 Cost: 0.029046 Accuracy 100.00%\n",
            "Epoch  670/1000 Cost: 0.028654 Accuracy 100.00%\n",
            "Epoch  680/1000 Cost: 0.028272 Accuracy 100.00%\n",
            "Epoch  690/1000 Cost: 0.027900 Accuracy 100.00%\n",
            "Epoch  700/1000 Cost: 0.027538 Accuracy 100.00%\n",
            "Epoch  710/1000 Cost: 0.027186 Accuracy 100.00%\n",
            "Epoch  720/1000 Cost: 0.026842 Accuracy 100.00%\n",
            "Epoch  730/1000 Cost: 0.026507 Accuracy 100.00%\n",
            "Epoch  740/1000 Cost: 0.026181 Accuracy 100.00%\n",
            "Epoch  750/1000 Cost: 0.025862 Accuracy 100.00%\n",
            "Epoch  760/1000 Cost: 0.025552 Accuracy 100.00%\n",
            "Epoch  770/1000 Cost: 0.025248 Accuracy 100.00%\n",
            "Epoch  780/1000 Cost: 0.024952 Accuracy 100.00%\n",
            "Epoch  790/1000 Cost: 0.024663 Accuracy 100.00%\n",
            "Epoch  800/1000 Cost: 0.024381 Accuracy 100.00%\n",
            "Epoch  810/1000 Cost: 0.024104 Accuracy 100.00%\n",
            "Epoch  820/1000 Cost: 0.023835 Accuracy 100.00%\n",
            "Epoch  830/1000 Cost: 0.023571 Accuracy 100.00%\n",
            "Epoch  840/1000 Cost: 0.023313 Accuracy 100.00%\n",
            "Epoch  850/1000 Cost: 0.023061 Accuracy 100.00%\n",
            "Epoch  860/1000 Cost: 0.022814 Accuracy 100.00%\n",
            "Epoch  870/1000 Cost: 0.022572 Accuracy 100.00%\n",
            "Epoch  880/1000 Cost: 0.022336 Accuracy 100.00%\n",
            "Epoch  890/1000 Cost: 0.022104 Accuracy 100.00%\n",
            "Epoch  900/1000 Cost: 0.021877 Accuracy 100.00%\n",
            "Epoch  910/1000 Cost: 0.021655 Accuracy 100.00%\n",
            "Epoch  920/1000 Cost: 0.021437 Accuracy 100.00%\n",
            "Epoch  930/1000 Cost: 0.021224 Accuracy 100.00%\n",
            "Epoch  940/1000 Cost: 0.021015 Accuracy 100.00%\n",
            "Epoch  950/1000 Cost: 0.020810 Accuracy 100.00%\n",
            "Epoch  960/1000 Cost: 0.020609 Accuracy 100.00%\n",
            "Epoch  970/1000 Cost: 0.020412 Accuracy 100.00%\n",
            "Epoch  980/1000 Cost: 0.020219 Accuracy 100.00%\n",
            "Epoch  990/1000 Cost: 0.020029 Accuracy 100.00%\n",
            "Epoch 1000/1000 Cost: 0.019843 Accuracy 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
        "y_data = [[0], [0], [0], [1], [1], [1]]\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)\n",
        "\n",
        "class BinaryClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(2,1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  def forward(self, x):\n",
        "    return self.sigmoid(self.linear(x))\n",
        "    \n",
        "\n",
        "model = BinaryClassifier()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1)\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs+1):\n",
        "  hypothesis = model(x_train)\n",
        "  cost = F.binary_cross_entropy(hypothesis, y_train)\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    prediction = hypothesis >= torch.FloatTensor([0.5])\n",
        "    correct_prediction = prediction.float() == y_train\n",
        "    accuracy = correct_prediction.sum().item()/len(correct_prediction)\n",
        "    print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
        "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
        "        ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_yea9wlqDhS",
        "outputId": "c40d50e9-0997-462d-fbed-488a1f5eb7a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/1000 Cost: 0.666565 Accuracy 50.00%\n",
            "Epoch   10/1000 Cost: 0.597682 Accuracy 83.33%\n",
            "Epoch   20/1000 Cost: 0.650253 Accuracy 83.33%\n",
            "Epoch   30/1000 Cost: 0.541721 Accuracy 83.33%\n",
            "Epoch   40/1000 Cost: 0.453505 Accuracy 83.33%\n",
            "Epoch   50/1000 Cost: 0.369330 Accuracy 83.33%\n",
            "Epoch   60/1000 Cost: 0.290070 Accuracy 83.33%\n",
            "Epoch   70/1000 Cost: 0.221421 Accuracy 83.33%\n",
            "Epoch   80/1000 Cost: 0.174404 Accuracy 100.00%\n",
            "Epoch   90/1000 Cost: 0.152180 Accuracy 100.00%\n",
            "Epoch  100/1000 Cost: 0.140640 Accuracy 100.00%\n",
            "Epoch  110/1000 Cost: 0.131313 Accuracy 100.00%\n",
            "Epoch  120/1000 Cost: 0.123176 Accuracy 100.00%\n",
            "Epoch  130/1000 Cost: 0.116007 Accuracy 100.00%\n",
            "Epoch  140/1000 Cost: 0.109642 Accuracy 100.00%\n",
            "Epoch  150/1000 Cost: 0.103953 Accuracy 100.00%\n",
            "Epoch  160/1000 Cost: 0.098838 Accuracy 100.00%\n",
            "Epoch  170/1000 Cost: 0.094214 Accuracy 100.00%\n",
            "Epoch  180/1000 Cost: 0.090014 Accuracy 100.00%\n",
            "Epoch  190/1000 Cost: 0.086181 Accuracy 100.00%\n",
            "Epoch  200/1000 Cost: 0.082669 Accuracy 100.00%\n",
            "Epoch  210/1000 Cost: 0.079439 Accuracy 100.00%\n",
            "Epoch  220/1000 Cost: 0.076459 Accuracy 100.00%\n",
            "Epoch  230/1000 Cost: 0.073699 Accuracy 100.00%\n",
            "Epoch  240/1000 Cost: 0.071136 Accuracy 100.00%\n",
            "Epoch  250/1000 Cost: 0.068750 Accuracy 100.00%\n",
            "Epoch  260/1000 Cost: 0.066523 Accuracy 100.00%\n",
            "Epoch  270/1000 Cost: 0.064439 Accuracy 100.00%\n",
            "Epoch  280/1000 Cost: 0.062484 Accuracy 100.00%\n",
            "Epoch  290/1000 Cost: 0.060648 Accuracy 100.00%\n",
            "Epoch  300/1000 Cost: 0.058919 Accuracy 100.00%\n",
            "Epoch  310/1000 Cost: 0.057287 Accuracy 100.00%\n",
            "Epoch  320/1000 Cost: 0.055746 Accuracy 100.00%\n",
            "Epoch  330/1000 Cost: 0.054287 Accuracy 100.00%\n",
            "Epoch  340/1000 Cost: 0.052905 Accuracy 100.00%\n",
            "Epoch  350/1000 Cost: 0.051592 Accuracy 100.00%\n",
            "Epoch  360/1000 Cost: 0.050344 Accuracy 100.00%\n",
            "Epoch  370/1000 Cost: 0.049157 Accuracy 100.00%\n",
            "Epoch  380/1000 Cost: 0.048025 Accuracy 100.00%\n",
            "Epoch  390/1000 Cost: 0.046945 Accuracy 100.00%\n",
            "Epoch  400/1000 Cost: 0.045914 Accuracy 100.00%\n",
            "Epoch  410/1000 Cost: 0.044928 Accuracy 100.00%\n",
            "Epoch  420/1000 Cost: 0.043984 Accuracy 100.00%\n",
            "Epoch  430/1000 Cost: 0.043079 Accuracy 100.00%\n",
            "Epoch  440/1000 Cost: 0.042212 Accuracy 100.00%\n",
            "Epoch  450/1000 Cost: 0.041380 Accuracy 100.00%\n",
            "Epoch  460/1000 Cost: 0.040580 Accuracy 100.00%\n",
            "Epoch  470/1000 Cost: 0.039811 Accuracy 100.00%\n",
            "Epoch  480/1000 Cost: 0.039071 Accuracy 100.00%\n",
            "Epoch  490/1000 Cost: 0.038359 Accuracy 100.00%\n",
            "Epoch  500/1000 Cost: 0.037673 Accuracy 100.00%\n",
            "Epoch  510/1000 Cost: 0.037011 Accuracy 100.00%\n",
            "Epoch  520/1000 Cost: 0.036372 Accuracy 100.00%\n",
            "Epoch  530/1000 Cost: 0.035755 Accuracy 100.00%\n",
            "Epoch  540/1000 Cost: 0.035159 Accuracy 100.00%\n",
            "Epoch  550/1000 Cost: 0.034583 Accuracy 100.00%\n",
            "Epoch  560/1000 Cost: 0.034026 Accuracy 100.00%\n",
            "Epoch  570/1000 Cost: 0.033487 Accuracy 100.00%\n",
            "Epoch  580/1000 Cost: 0.032964 Accuracy 100.00%\n",
            "Epoch  590/1000 Cost: 0.032459 Accuracy 100.00%\n",
            "Epoch  600/1000 Cost: 0.031968 Accuracy 100.00%\n",
            "Epoch  610/1000 Cost: 0.031492 Accuracy 100.00%\n",
            "Epoch  620/1000 Cost: 0.031031 Accuracy 100.00%\n",
            "Epoch  630/1000 Cost: 0.030583 Accuracy 100.00%\n",
            "Epoch  640/1000 Cost: 0.030147 Accuracy 100.00%\n",
            "Epoch  650/1000 Cost: 0.029725 Accuracy 100.00%\n",
            "Epoch  660/1000 Cost: 0.029314 Accuracy 100.00%\n",
            "Epoch  670/1000 Cost: 0.028914 Accuracy 100.00%\n",
            "Epoch  680/1000 Cost: 0.028525 Accuracy 100.00%\n",
            "Epoch  690/1000 Cost: 0.028147 Accuracy 100.00%\n",
            "Epoch  700/1000 Cost: 0.027779 Accuracy 100.00%\n",
            "Epoch  710/1000 Cost: 0.027420 Accuracy 100.00%\n",
            "Epoch  720/1000 Cost: 0.027070 Accuracy 100.00%\n",
            "Epoch  730/1000 Cost: 0.026730 Accuracy 100.00%\n",
            "Epoch  740/1000 Cost: 0.026398 Accuracy 100.00%\n",
            "Epoch  750/1000 Cost: 0.026074 Accuracy 100.00%\n",
            "Epoch  760/1000 Cost: 0.025758 Accuracy 100.00%\n",
            "Epoch  770/1000 Cost: 0.025450 Accuracy 100.00%\n",
            "Epoch  780/1000 Cost: 0.025149 Accuracy 100.00%\n",
            "Epoch  790/1000 Cost: 0.024855 Accuracy 100.00%\n",
            "Epoch  800/1000 Cost: 0.024568 Accuracy 100.00%\n",
            "Epoch  810/1000 Cost: 0.024288 Accuracy 100.00%\n",
            "Epoch  820/1000 Cost: 0.024014 Accuracy 100.00%\n",
            "Epoch  830/1000 Cost: 0.023746 Accuracy 100.00%\n",
            "Epoch  840/1000 Cost: 0.023484 Accuracy 100.00%\n",
            "Epoch  850/1000 Cost: 0.023228 Accuracy 100.00%\n",
            "Epoch  860/1000 Cost: 0.022978 Accuracy 100.00%\n",
            "Epoch  870/1000 Cost: 0.022733 Accuracy 100.00%\n",
            "Epoch  880/1000 Cost: 0.022493 Accuracy 100.00%\n",
            "Epoch  890/1000 Cost: 0.022258 Accuracy 100.00%\n",
            "Epoch  900/1000 Cost: 0.022028 Accuracy 100.00%\n",
            "Epoch  910/1000 Cost: 0.021803 Accuracy 100.00%\n",
            "Epoch  920/1000 Cost: 0.021582 Accuracy 100.00%\n",
            "Epoch  930/1000 Cost: 0.021366 Accuracy 100.00%\n",
            "Epoch  940/1000 Cost: 0.021154 Accuracy 100.00%\n",
            "Epoch  950/1000 Cost: 0.020946 Accuracy 100.00%\n",
            "Epoch  960/1000 Cost: 0.020743 Accuracy 100.00%\n",
            "Epoch  970/1000 Cost: 0.020543 Accuracy 100.00%\n",
            "Epoch  980/1000 Cost: 0.020347 Accuracy 100.00%\n",
            "Epoch  990/1000 Cost: 0.020155 Accuracy 100.00%\n",
            "Epoch 1000/1000 Cost: 0.019967 Accuracy 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "z = torch.FloatTensor([1,2,3])\n",
        "hypothesis = F.softmax(z, dim = 0)\n",
        "print(hypothesis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaFbvgOLNpE3",
        "outputId": "98325153-12b0-464a-de03-962e17408003"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0900, 0.2447, 0.6652])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "z = torch.rand(3,5, requires_grad = True)\n",
        "hypothesis = F.softmax(z, dim = 1)\n",
        "hypothesis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuultnR2OGf8",
        "outputId": "6080960c-c537-4a92-f62e-ea4d03bfbbd1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
              "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
              "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbX4XCJkP7qF",
        "outputId": "9967ee74-41b5-4228-a4a9-19ff31250a6c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.randint(5, (3,)).long()\n",
        "print(y+1)\n",
        "print(y.unsqueeze(1))\n",
        "y_one_hot = torch.zeros_like(hypothesis)\n",
        "y_one_hot.scatter_(1, y.unsqueeze(1),1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWjisjOnQ-CQ",
        "outputId": "e2d246dc-a779-42ce-e0db-a1c047025683"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4, 5, 3])\n",
            "tensor([[3],\n",
            "        [4],\n",
            "        [2]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 1.],\n",
              "        [0., 0., 1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cost = (y_one_hot * - torch.log(hypothesis)).sum(dim = 1).mean()\n",
        "print(cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r132Cg-nSIg0",
        "outputId": "582233fc-f519-4ee3-8f71-3498fac38862"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.5291, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    }
  ]
}